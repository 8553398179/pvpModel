{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b33ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.0 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'C:/Users/darshanRaghunath/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Feb 23 02:11:18 2022\n",
    "\n",
    "@author: darshanRaghunath\n",
    "\"\"\"\n",
    "\n",
    "# -*- coding: utf-8 \n",
    "\n",
    "import pandas as pd\n",
    "df1 = pd.read_csv(\"C:/Users/darshanRaghunath/bookKalyan1.csv\")\n",
    "col=[\"8\",\"9\"]\n",
    "df1 = df1[col]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df2 = pd.read_csv(\"C:/Users/darshanRaghunath/book19.csv\")\n",
    "df2 = df2[col]\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "vocab_size = 5000\n",
    "embedding_dim = 64\n",
    "max_length = 50\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'\n",
    "training_portion = .7\n",
    "num_epochs = 8\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(df.head())\n",
    "\n",
    "df1.columns = ['Paragraph', 'class']\n",
    "df2.columns = ['Paragraph', 'class']\n",
    "#print(df.head())\n",
    "#print(df.info())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "#stratify=df['class']\n",
    "\n",
    "\n",
    "\n",
    "articles = []\n",
    "labels = []\n",
    "count=1\n",
    "\n",
    "\n",
    "\n",
    "noise =0\n",
    "for i  in df2[\"Paragraph\"]:\n",
    "        noise += 1\n",
    "        articles.append(i)    \n",
    "        if(noise> 3400):\n",
    "            break\n",
    "        \n",
    "        \n",
    "noise =0    \n",
    "for i  in df1[\"Paragraph\"]:\n",
    "        noise += 1\n",
    "        articles.append(i)    \n",
    "        if(noise>6900):\n",
    "            break\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "noise =0        \n",
    "        \n",
    "print(len(articles))     \n",
    "for i in df2[\"class\"]:\n",
    "    l=[]\n",
    "    noise += 1\n",
    "    l.append(i)\n",
    "    labels.append(l)\n",
    "    if(noise> 3400):\n",
    "        break\n",
    "    \n",
    "noise =0   \n",
    "    \n",
    "for i in df1[\"class\"]:\n",
    "    l=[]\n",
    "    noise += 1\n",
    "    l.append(i)\n",
    "    labels.append(l)\n",
    "    if(noise> 6900):\n",
    "        break\n",
    "    \n",
    "    \n",
    "print(len(labels))\n",
    "\n",
    "\n",
    "\n",
    "#print(labels[100])\n",
    "#print(articles[100])\n",
    "\n",
    "train_articles,validation_articles , train_labels,validation_labels  = train_test_split(articles,labels,test_size=(1-training_portion),random_state = 100)\n",
    "\n",
    "\n",
    "evalSize = int(len(validation_articles) * 0.3)\n",
    "\n",
    "\n",
    "evaluate_articles = validation_articles [0:evalSize]\n",
    "evaluate_labels = validation_labels[0:evalSize]\n",
    "                                    \n",
    "validation_articles= validation_articles[evalSize :]\n",
    "validation_labels =  validation_labels[evalSize :]\n",
    "\n",
    "\n",
    "\n",
    "print(\"Validation\")\n",
    "print(len(validation_articles))\n",
    "\n",
    "\n",
    "print(\"Evaluation\")\n",
    "print(len(evaluate_articles))\n",
    "\n",
    "'''\n",
    "\n",
    "train_size = int(len(articles) * training_portion)\n",
    "train_articles = articles[0: train_size]\n",
    "train_labels = labels[0: train_size]\n",
    "\n",
    "validation_articles = articles[train_size:]\n",
    "validation_labels = labels[train_size:]\n",
    "\n",
    "#print(len(validation_labels[0]))\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "#print(train_size)\n",
    "print(\"train and validation\")\n",
    "\n",
    "print(len(train_articles))\n",
    "print(len(train_labels))\n",
    "print(len(validation_articles))\n",
    "print(len(validation_labels))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok,filters='+-',)\n",
    "tokenizer.fit_on_texts(train_articles)\n",
    "word_index = tokenizer.word_index\n",
    "#print(word_index)\n",
    "#dict(list(word_index.items())[0:10])\n",
    "\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_articles)\n",
    "#print(train_sequences[100])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "print(len(train_sequences[0]))\n",
    "print(len(train_padded[0]))\n",
    "\n",
    "print(len(train_sequences[1]))\n",
    "print(len(train_padded[1]))\n",
    "\n",
    "print(len(train_sequences[10]))\n",
    "print(len(train_padded[10]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "validation_sequences = tokenizer.texts_to_sequences(validation_articles)\n",
    "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "\n",
    "\n",
    "evaluate_sequences = tokenizer.texts_to_sequences(evaluate_articles)\n",
    "evaluate_padded = pad_sequences(evaluate_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "print(len(validation_sequences))\n",
    "print(validation_padded.shape)\n",
    "\n",
    "\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_article(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "print(decode_article(train_padded[100]))\n",
    "print('---')\n",
    "print(train_articles[100])\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "print(decode_article(train_padded[5005]))\n",
    "print('---')\n",
    "print(train_articles[5005])\n",
    "\n",
    "\n",
    "\n",
    "train_labels =np.array(train_labels)\n",
    "validation_labels = np.array(validation_labels)\n",
    "\n",
    "\n",
    "\n",
    "evaluate_labels= np.array(evaluate_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df['Paragraph'], df['class'],test_size=0.1)\n",
    "#print(X_train[10:20])\n",
    "#print(y_train[10:20])\n",
    "\n",
    "\n",
    "train = df[0:3000]\n",
    "test = df[3000:]\n",
    "print(train.head())\n",
    "print(train.info())\n",
    "\n",
    "print(test.head())\n",
    "print(test.info())\n",
    "\n",
    "\n",
    "#print(train[0])\n",
    "\n",
    "#print(len(X_train)) \n",
    "#print(type(X_train[2]))\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ebafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, None, 64)          320000    \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 128)              66048     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 394,759\n",
      "Trainable params: 394,759\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/8\n",
      "226/226 - 25s - loss: 0.7052 - accuracy: 0.7749 - val_loss: 0.2244 - val_accuracy: 0.9418 - 25s/epoch - 112ms/step\n",
      "Epoch 2/8\n",
      "226/226 - 25s - loss: 0.1337 - accuracy: 0.9669 - val_loss: 0.1489 - val_accuracy: 0.9570 - 25s/epoch - 112ms/step\n",
      "Epoch 3/8\n",
      "226/226 - 27s - loss: 0.0690 - accuracy: 0.9834 - val_loss: 0.1747 - val_accuracy: 0.9543 - 27s/epoch - 117ms/step\n",
      "Epoch 4/8\n",
      "226/226 - 29s - loss: 0.0480 - accuracy: 0.9882 - val_loss: 0.1620 - val_accuracy: 0.9621 - 29s/epoch - 126ms/step\n",
      "Epoch 5/8\n",
      "226/226 - 28s - loss: 0.0349 - accuracy: 0.9922 - val_loss: 0.1708 - val_accuracy: 0.9579 - 28s/epoch - 125ms/step\n",
      "Epoch 6/8\n",
      "226/226 - 28s - loss: 0.0336 - accuracy: 0.9911 - val_loss: 0.1920 - val_accuracy: 0.9487 - 28s/epoch - 126ms/step\n",
      "Epoch 7/8\n",
      "226/226 - 29s - loss: 0.0289 - accuracy: 0.9929 - val_loss: 0.1778 - val_accuracy: 0.9579 - 29s/epoch - 129ms/step\n",
      "Epoch 8/8\n",
      "226/226 - 29s - loss: 0.0277 - accuracy: 0.9924 - val_loss: 0.1909 - val_accuracy: 0.9603 - 29s/epoch - 127ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses, lstm_cell_14_layer_call_fn, lstm_cell_14_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: E:/model/pvp\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: E:/model/pvp\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000001DCA0A84250> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000001DCA0A7E520> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 65   0   0   4   1   0]\n",
      " [  0  28   0   0   1   0]\n",
      " [  0   1  58   1   1   0]\n",
      " [ 11   1   1 383   1   0]\n",
      " [  0   1   1   4 326   0]\n",
      " [  0   0   0   0   0  38]]\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # Add an Embedding layer expecting input vocab of size 5000, and output embedding dimension of size 64 we set at the top\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    # use ReLU in place of tanh function since they are very good alternatives of each other.\n",
    "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
    "    # Add a Dense layer with 6 units and softmax activation.\n",
    "    # When we have multiple outputs, softmax convert outputs layers into a probability distribution.\n",
    "    tf.keras.layers.Dense(7, activation='softmax')\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "#print(train_labels.size)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(train_padded, train_labels, epochs=num_epochs, validation_data=(validation_padded, validation_labels), verbose=2)\n",
    "\n",
    "model.save(\"E:/model/pvp\")\n",
    "\n",
    "'''\n",
    "y_prd =model.predict(validation_padded)\n",
    "print(len(validation_padded))\n",
    "print(len(y_prd))\n",
    "print(validation_labels[0])\n",
    "#y_prd1 =np.argmax(y_prd)\n",
    "print(np.argmax(y_prd[0]))\n",
    "#print(confusion_matrix(validation_labels, y_prd1))\n",
    "'''\n",
    "\n",
    "\n",
    "y_prd =model.predict(evaluate_padded)\n",
    "#print(len(validation_padded))\n",
    "#print(len(y_prd))\n",
    "#print(validation_labels[0])\n",
    "#y_prd1 =np.argmax(y_prd)\n",
    "#print(np.argmax(y_prd[0]))\n",
    "#print(confusion_matrix(validation_labels, y_prd1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ycorrect = []\n",
    "ypred = []\n",
    "\n",
    "for i in range(len(evaluate_padded)):\n",
    "    ycorrect.append(evaluate_labels[i][0])\n",
    "    ypred.append(np.argmax(y_prd[i]))\n",
    "    \n",
    "    \n",
    "print(confusion_matrix(ycorrect, ypred ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cb9d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9285714285714286, 0.9655172413793104, 0.9508196721311475, 0.964735516372796, 0.9819277108433735, 1.0]\n"
     ]
    }
   ],
   "source": [
    "mat = confusion_matrix(ycorrect, ypred )\n",
    "\n",
    "percent=[]\n",
    "for i in range(len(mat)):\n",
    "    denum=0\n",
    "    num=0\n",
    "    for j in range(len(mat[i])):\n",
    "        if(i ==j):\n",
    "            num = mat[i][i]\n",
    "        \n",
    "        denum += mat[i][j]\n",
    "    percent.append(num/denum)\n",
    "        \n",
    "print(percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f617e54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title 0.9285714285714286\n",
      "Author 0.9655172413793104\n",
      "Affiliation 0.9508196721311475\n",
      "Abstract 0.964735516372796\n",
      "Noise 0.9819277108433735\n",
      "Image 1.0\n"
     ]
    }
   ],
   "source": [
    "output =[\"Title\", \"Author\", \"Affiliation\" , \"Abstract\", \"Noise\", \"Image\" ,\"ID's\"]\n",
    "\n",
    "for i in range(len(percent)):\n",
    "    print(output[i], percent[i])\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
